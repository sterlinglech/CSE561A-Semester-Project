{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92190fe1",
   "metadata": {},
   "source": [
    "# Possible models to use\n",
    "\n",
    "## DistilBART - distilled version of BART, which is much smaller than the full BART model but retains much of its performance. Since it is distilled, it's faster and more efficient while still being well-suited for summarization tasks. DistilBART is designed for text summarization, and the cnn-12-6 variant is trained on news articles, making it a viable medium sized model for summarizing legal documents.\n",
    "\n",
    "## T5 (Text-to-Text Transfer Transformer) - Small or Base - T5 treats every task as a text-to-text problem, making it very flexible for summarization. The small and base variants offer a middle ground between performance and model size, making them suitable for use cases where computational resources are limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312bc7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df84a5ae",
   "metadata": {},
   "source": [
    "### Here I load the datasets and edit some of the columns prior to tokenizing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecc784f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 7773\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n",
      "Dataset({\n",
      "    features: ['context', 'endings', 'labels'],\n",
      "    num_rows: 45000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 9000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 9000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 55000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 60000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "ds1_train = load_dataset(\"joelniklaus/legal_case_document_summarization\", split='train')\n",
    "ds1_train = ds1_train.remove_columns(['dataset_name'])\n",
    "ds1_train = ds1_train.rename_column('judgement', 'text')\n",
    "ds1_train = ds1_train.rename_column('summary', 'labels')\n",
    "print(ds1_train)\n",
    "\n",
    "ds1_test = load_dataset(\"joelniklaus/legal_case_document_summarization\", split='test')\n",
    "ds1_test = ds1_test.remove_columns(['dataset_name'])\n",
    "ds1_test = ds1_test.rename_column('judgement', 'text')\n",
    "ds1_test = ds1_test.rename_column('summary', 'labels')\n",
    "\n",
    "# NOTE: This dataset only has 50 rows. It may not be a dataset we want to use.\n",
    "# NOTE: THIS DATA IS NOT PLAYING NICELY WITH CONCATENATION\n",
    "# Although the summaries appear to be good\n",
    "ds2_train = load_dataset(\"manasvikalyan/legal-documents-summary\")\n",
    "ds2_train = ds2_train['data']\n",
    "ds2_train = ds2_train.remove_columns(['summary_a2'])\n",
    "ds2_train = ds2_train.rename_column('summary_a1', 'labels')\n",
    "ds2_train = ds2_train.rename_column('judgement', 'text')\n",
    "print(ds2_train)\n",
    "\n",
    "# NOTE: This dataset may not be useful the Task: Text Summarization. But moreso, option selection.\n",
    "# Context: is a given legal scenario or fact pattern\n",
    "# Options (Holdings): Multiple candidate holdings, one of which is correct.\n",
    "# Labels: The correct holding is labeled to allow supervised learning and evaluation\n",
    "ds3_train = load_dataset(\"coastalcph/lex_glue\", \"case_hold\", split='train')\n",
    "ds3_train = ds3_train.rename_column('label', 'labels')\n",
    "ds3_test = load_dataset(\"coastalcph/lex_glue\", \"case_hold\", split='test')\n",
    "print(ds3_train)\n",
    "\n",
    "ds4_train = load_dataset(\"coastalcph/lex_glue\", \"ecthr_a\", split='train')\n",
    "ds4_test = load_dataset(\"coastalcph/lex_glue\", \"ecthr_a\", split='test')\n",
    "print(ds4_train)\n",
    "\n",
    "ds5_train = load_dataset(\"coastalcph/lex_glue\", \"ecthr_b\", split='train')\n",
    "ds5_test = load_dataset(\"coastalcph/lex_glue\", \"ecthr_b\", split='test')\n",
    "print(ds5_train)\n",
    "\n",
    "ds6_train = load_dataset(\"coastalcph/lex_glue\", \"eurlex\", split='train')\n",
    "ds6_test = load_dataset(\"coastalcph/lex_glue\", \"eurlex\", split='test')\n",
    "print(ds6_train)\n",
    "\n",
    "ds7_train = load_dataset(\"coastalcph/lex_glue\", \"ledgar\", split='train')\n",
    "ds7_train = ds7_train.rename_column('label', 'labels')\n",
    "ds7_test = load_dataset(\"coastalcph/lex_glue\", \"ledgar\", split='test')\n",
    "print(ds7_train)\n",
    "\n",
    "ds8_train = load_dataset(\"coastalcph/lex_glue\", \"scotus\", split='train')\n",
    "ds8_train = ds8_train.rename_column('label', 'labels')\n",
    "ds8_test = load_dataset(\"coastalcph/lex_glue\", \"scotus\", split='test')\n",
    "print(ds8_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d1aa5e",
   "metadata": {},
   "source": [
    "### Here I am pre-processing the data for the DistilBART model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33c6b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "687cc31e-c81c-4613-891f-7c61aba144cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14301ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function for text and summaries\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(examples['text'], max_length=512, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Tokenize the output summary labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['labels'], max_length=150, truncation=True, padding='max_length')\n",
    "\n",
    "    # Set the tokenized labels in the input dictionary\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "def tokenize_function_for_ds4(examples):\n",
    "    # Tokenize each item in the list of 'text' entries\n",
    "    inputs = tokenizer(\n",
    "        examples['text'],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        is_split_into_words=True  # Add this if each item is already tokenized/split into words\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "def tokenize_function_for_ds6(examples):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(examples['text'], max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "    # If labels are in batches, process accordingly\n",
    "    if 'label' in examples:\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                [str(label) for label in examples['label']],\n",
    "                max_length=150,\n",
    "                truncation=True,\n",
    "                padding='max_length'\n",
    "            )\n",
    "        inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "def tokenize_function_for_ds7(examples):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(examples['text'], max_length=512, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Convert labels to strings if necessary\n",
    "    labels = [str(label) for label in examples['label']]\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tokenized_labels = tokenizer(labels, max_length=150, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Set the tokenized labels in the input dictionary\n",
    "    inputs['labels'] = tokenized_labels['input_ids']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812264cd",
   "metadata": {},
   "source": [
    "### Here I am just Tokenizing 'ds1' and 'ds2' for DistilBART (ds1_train and ds2_actual)\n",
    "\n",
    "### TODO: Tokenize the training set data later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dfffdba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a71413319364b4fac28198106810554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7773 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Tokenize the datasets for DistilBART\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Training Data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ds1_train_tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mds1_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m ds2_train_tokenized \u001b[38;5;241m=\u001b[39m ds2_train\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# ds3_train_tokenized = ds3_train.map(tokenize_function, batched=True) <-- multiple choice data\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py:3035\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3031\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3032\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3033\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3034\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3035\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3036\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3037\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py:3438\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3434\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3435\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3436\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3438\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3442\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3444\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3447\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py:3300\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3299\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3300\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3302\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3303\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3304\u001b[0m     }\n",
      "Cell \u001b[0;32mIn [35], line 8\u001b[0m, in \u001b[0;36mtokenize_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Tokenize the output summary labels\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mas_target_tokenizer():\n\u001b[0;32m----> 8\u001b[0m     labels \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Set the tokenized labels in the input dictionary\u001b[39;00m\n\u001b[1;32m     11\u001b[0m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/datasets/formatting/formatting.py:277\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 277\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[1;32m    279\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "# Tokenize the datasets for DistilBART\n",
    "# Training Data\n",
    "ds1_train_tokenized = ds1_train.map(tokenize_function, batched=True)\n",
    "ds2_train_tokenized = ds2_train.map(tokenize_function, batched=True)\n",
    "# ds3_train_tokenized = ds3_train.map(tokenize_function, batched=True) <-- multiple choice data\n",
    "ds4_train_tokenized = ds4_train.map(tokenize_function_for_ds4, batched=True)\n",
    "ds5_train_tokenized = ds5_train.map(tokenize_function_for_ds4, batched=True)\n",
    "ds6_train_tokenized = ds6_train.map(tokenize_function_for_ds6, batched=True)\n",
    "ds7_train_tokenized = ds7_train.map(tokenize_function_for_ds7, batched=True)\n",
    "ds8_train_tokenized = ds8_train.map(tokenize_function_for_ds7, batched=True)\n",
    "\n",
    "# Testing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82fe788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd345cbd",
   "metadata": {},
   "source": [
    "### TODO: set the other dataset formats later:\n",
    "\n",
    "### Extra Columns (`input_ids`, `attention_mask`, `labels`)\n",
    "- **`input_ids`**: Token IDs representing the input text for the model.\n",
    "- **`attention_mask`**: Identifies which tokens are real and which are padding.\n",
    "- **`labels`**: Token IDs representing the target summary, used for training.\n",
    "These columns are essential for the model to properly process inputs, ignore padding, and learn to generate correct summaries during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba777233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset format to PyTorch tensors\n",
    "# print(ds1_train_tokenized)\n",
    "ds1_train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "ds2_train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "# ds3_train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "ds4_train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "ds5_train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "ds6_train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "ds7_train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "ds8_train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "# print(ds1_train_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd4efb",
   "metadata": {},
   "source": [
    "### Here I am concatenating the datasets to use all together\n",
    "\n",
    "### TODO: concatenate the rest of the datasets later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56941f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate/Merge the datasets\n",
    "# FIX LATER --> combined_dataset = concatenate_datasets([ds1_train_tokenized, ds2_actual_tokenized])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e92e97",
   "metadata": {},
   "source": [
    "### Splitting the combined dataset into train and validation sets\n",
    "\n",
    "### TODO: split all datasets here later. Not just ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07068ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX LATER --> combined_dataset = combined_dataset.train_test_split(test_size=0.2)\n",
    "ds1_train_tokenized = ds1_train_tokenized.train_test_split(test_size=0.2)\n",
    "train_dataset = ds1_train_tokenized['train'] \n",
    "val_dataset = ds1_train_tokenized['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633966e9",
   "metadata": {},
   "source": [
    "### Load the DistilBART model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d0ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c10551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DistilBART model for conditional generation\n",
    "model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20545c",
   "metadata": {},
   "source": [
    "### Setting up training arguments for the model here\n",
    "\n",
    "### TODO: These can be modified later to improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e709e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',            # output directory\n",
    "    eval_strategy=\"epoch\",       # evaluate at each epoch\n",
    "    learning_rate=5e-5,                # learning rate\n",
    "    per_device_train_batch_size=4,     # batch size for training\n",
    "    per_device_eval_batch_size=4,      # batch size for evaluation\n",
    "    num_train_epochs=3,                # number of training epochs\n",
    "    weight_decay=0.01,                 # strength of weight decay\n",
    "    save_total_limit=2,                # only keep last 2 checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c29b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3025527f",
   "metadata": {},
   "source": [
    "### Training the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e43ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c50b6c",
   "metadata": {},
   "source": [
    "### Training and Evaluation Results\n",
    "\n",
    "After training the DistilBART model for **3 epochs** on the legal case summarization dataset, we achieved the following results:\n",
    "\n",
    "#### Training Metrics:\n",
    "- **Training Loss**: **1.8569**\n",
    "  - The training loss represents the average difference between the predicted token probabilities and the actual tokens across the entire dataset. For a complex task like summarization, this loss value indicates that the model is learning effectively.\n",
    "  - While ideally a loss closer to zero is better, for sequence generation tasks involving long and complex legal texts, a value around **1.8** is reasonable. The model is capturing the patterns within the legal data without significant overfitting.\n",
    "\n",
    "#### Evaluation Metrics:\n",
    "- **Evaluation Loss**: **1.9931**\n",
    "  - The evaluation loss is slightly higher than the training loss, which suggests that the model generalizes moderately well to unseen data. This is a positive sign as it implies that the model has not overfit significantly to the training dataset.\n",
    "  - Summarization models, particularly with large input/output sequences and complex legal terminology, typically have evaluation loss values greater than **1**. The small difference between the training and evaluation loss indicates good generalization.\n",
    "\n",
    "- **Evaluation Runtime**: **3,726.99 seconds** (~62 minutes)\n",
    "  - This is the time taken to evaluate the model over the validation set. The runtime is reasonable considering the complexity of the task and the length of the input sequences.\n",
    "\n",
    "- **Samples per Second**:\n",
    "  - **Training**: **0.407** samples per second\n",
    "  - **Evaluation**: **0.417** samples per second\n",
    "  - These rates are consistent across training and evaluation, indicating that the model was trained and evaluated with stable performance given the computational resources. The relatively low samples per second can be attributed to the complexity of processing long legal documents and generating summaries.\n",
    "\n",
    "#### Interpretation of Loss Values:\n",
    "- **Training Loss and Evaluation Loss**:\n",
    "  - The **training loss of 1.8569** compared to the **evaluation loss of 1.9931** indicates that the model is not significantly overfitting to the training set, which is a good outcome. The slight increase in evaluation loss shows that the model is encountering some additional complexity when dealing with unseen data, which is expected.\n",
    "  - In general, for summarization tasks involving complex data, a loss in the range of **1.5 - 3.0** is typical. This is due to the nature of cross-entropy loss accumulating over long sequences of tokens. Thus, the current loss values are quite reasonable.\n",
    "\n",
    "#### Next Steps for Improvement:\n",
    "1. **Hyperparameter Tuning**:\n",
    "   - Consider adjusting the learning rate or using **scheduled learning rate decay** to help further reduce the training and evaluation loss.\n",
    "2. **Additional Training Epochs**:\n",
    "   - Training for an additional **1-2 epochs** could further reduce the loss, provided that overfitting is controlled.\n",
    "3. **Regularization Techniques**:\n",
    "   - **Weight Decay** or **Dropout** could be introduced to help improve generalization.\n",
    "4. **Evaluate with ROUGE Metric**:\n",
    "   - In addition to using loss as a performance measure, evaluating the model with **ROUGE** scores can give a more targeted assessment of how well the summaries capture the important content from the legal texts.\n",
    "\n",
    "#### Summary:\n",
    "- The **training and evaluation losses** are reasonable for a text generation task involving legal documents. The model seems to be learning effectively without significant overfitting.\n",
    "- Further improvement can be achieved through hyperparameter tuning, training for additional epochs, and using metrics such as **ROUGE** to better evaluate the quality of the generated summaries.\n",
    "\n",
    "The next logical step is to test the quality of the generated summaries by comparing them with the reference summaries and calculating relevant metrics to better understand the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6995605e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
