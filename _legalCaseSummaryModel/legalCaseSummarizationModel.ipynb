{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963e8a53",
   "metadata": {},
   "source": [
    "# Possible models to use\n",
    "\n",
    "## DistilBART - distilled version of BART, which is much smaller than the full BART model but retains much of its performance. Since it is distilled, it's faster and more efficient while still being well-suited for summarization tasks. DistilBART is designed for text summarization, and the cnn-12-6 variant is trained on news articles, making it a viable medium sized model for summarizing legal documents.\n",
    "\n",
    "## T5 (Text-to-Text Transfer Transformer) - Small or Base - T5 treats every task as a text-to-text problem, making it very flexible for summarization. The small and base variants offer a middle ground between performance and model size, making them suitable for use cases where computational resources are limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312bc7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc784f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 7773\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 50\n",
      "})\n",
      "Dataset({\n",
      "    features: ['context', 'endings', 'label'],\n",
      "    num_rows: 45000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 9000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 9000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 55000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 60000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "ds1_train = load_dataset(\"joelniklaus/legal_case_document_summarization\", split='train')\n",
    "ds1_train = ds1_train.remove_columns(['dataset_name'])\n",
    "ds1_train = ds1_train.rename_column('judgement', 'text')\n",
    "ds1_train = ds1_train.rename_column('summary', 'label')\n",
    "print(ds1_train)\n",
    "\n",
    "# NOTE: This dataset only has 50 rows. It may not be a dataset we want to use.\n",
    "# Although the summaries appear to be good\n",
    "ds2_DatasetDict = load_dataset(\"manasvikalyan/legal-documents-summary\")\n",
    "ds2_actual = ds2_DatasetDict['data']\n",
    "ds2_actual = ds2_actual.remove_columns(['summary_a2'])\n",
    "ds2_actual = ds2_actual.rename_column('summary_a1', 'label')\n",
    "ds2_actual = ds2_actual.rename_column('judgement', 'text')\n",
    "print(ds2_actual)\n",
    "\n",
    "ds3_train = load_dataset(\"coastalcph/lex_glue\", \"case_hold\", split='train')\n",
    "print(ds3_train)\n",
    "ds4_train = load_dataset(\"coastalcph/lex_glue\", \"ecthr_a\", split='train')\n",
    "print(ds4_train)\n",
    "ds5_train = load_dataset(\"coastalcph/lex_glue\", \"ecthr_b\", split='train')\n",
    "print(ds5_train)\n",
    "ds6_train = load_dataset(\"coastalcph/lex_glue\", \"eurlex\", split='train')\n",
    "print(ds6_train)\n",
    "ds7_train = load_dataset(\"coastalcph/lex_glue\", \"ledgar\", split='train')\n",
    "print(ds7_train)\n",
    "ds8_train = load_dataset(\"coastalcph/lex_glue\", \"scotus\", split='train')\n",
    "print(ds8_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33c6b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "687cc31e-c81c-4613-891f-7c61aba144cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14301ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a tokenization function for the summarization task\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the 'text' (input legal case) and 'label' (summary)\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4135df89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 7773\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds1_train_tokenized = ds1_train.map(tokenize_function, batched=True)\n",
    "ds2_actual_tokenized = ds2_actual.map(tokenize_function, batched=True)\n",
    "\n",
    "print(ds1_train_tokenized)\n",
    "print(ds2_actual_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfffdba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the summarization task, the labels need to be tokenized separately\n",
    "def tokenize_labels(examples):\n",
    "    return tokenizer(examples['label'], padding=\"max_length\", truncation=True, max_length=150) # max_length can be adjusted for summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac085575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 7773\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Apply the tokenization to the label column (summaries)\n",
    "ds1_train_tokenized = ds1_train_tokenized.map(tokenize_labels, batched=True, remove_columns=['label'])\n",
    "ds2_actual_tokenized = ds2_actual_tokenized.map(tokenize_labels, batched=True, remove_columns=['label'])\n",
    "\n",
    "print(ds1_train_tokenized)\n",
    "print(ds2_actual_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56941f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 7773\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds1_train_tokenized = ds1_train_tokenized.rename_column(\"input_ids\", \"labels\")\n",
    "ds2_actual_tokenized = ds2_actual_tokenized.rename_column(\"input_ids\", \"labels\")\n",
    "\n",
    "print(ds1_train_tokenized)\n",
    "print(ds2_actual_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07068ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3fdde6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
