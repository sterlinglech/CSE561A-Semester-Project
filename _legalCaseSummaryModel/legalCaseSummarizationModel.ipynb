{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92190fe1",
   "metadata": {},
   "source": [
    "# Possible models to use\n",
    "\n",
    "## DistilBART - distilled version of BART, which is much smaller than the full BART model but retains much of its performance. Since it is distilled, it's faster and more efficient while still being well-suited for summarization tasks. DistilBART is designed for text summarization, and the cnn-12-6 variant is trained on news articles, making it a viable medium sized model for summarizing legal documents.\n",
    "\n",
    "## T5 (Text-to-Text Transfer Transformer) - Small or Base - T5 treats every task as a text-to-text problem, making it very flexible for summarization. The small and base variants offer a middle ground between performance and model size, making them suitable for use cases where computational resources are limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312bc7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df84a5ae",
   "metadata": {},
   "source": [
    "### Here I load the datasets and edit some of the columns prior to tokenizing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ecc784f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 7773\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n",
      "Dataset({\n",
      "    features: ['context', 'endings', 'labels'],\n",
      "    num_rows: 45000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 9000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 9000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 55000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 60000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "ds1_train = load_dataset(\"joelniklaus/legal_case_document_summarization\", split='train')\n",
    "ds1_train = ds1_train.remove_columns(['dataset_name'])\n",
    "ds1_train = ds1_train.rename_column('judgement', 'text')\n",
    "ds1_train = ds1_train.rename_column('summary', 'labels')\n",
    "print(ds1_train)\n",
    "\n",
    "ds1_test = load_dataset(\"joelniklaus/legal_case_document_summarization\", split='test')\n",
    "ds1_test = ds1_test.remove_columns(['dataset_name'])\n",
    "ds1_test = ds1_test.rename_column('judgement', 'text')\n",
    "ds1_test = ds1_test.rename_column('summary', 'labels')\n",
    "\n",
    "# NOTE: This dataset only has 50 rows. It may not be a dataset we want to use.\n",
    "# NOTE: THIS DATA IS NOT PLAYING NICELY WITH CONCATENATION\n",
    "# Although the summaries appear to be good\n",
    "ds2 = load_dataset(\"manasvikalyan/legal-documents-summary\")\n",
    "ds2 = ds2['data']\n",
    "ds2 = ds2.remove_columns(['summary_a2'])\n",
    "ds2 = ds2.rename_column('summary_a1', 'labels')\n",
    "ds2 = ds2.rename_column('judgement', 'text')\n",
    "print(ds2)\n",
    "\n",
    "# TODO: need to split this dataset manually later\n",
    "\n",
    "# NOTE: This dataset may not be useful the Task: Text Summarization. But moreso, option selection.\n",
    "# Context: is a given legal scenario or fact pattern\n",
    "# Options (Holdings): Multiple candidate holdings, one of which is correct.\n",
    "# Labels: The correct holding is labeled to allow supervised learning and evaluation\n",
    "ds3_train = load_dataset(\"coastalcph/lex_glue\", \"case_hold\", split='train')\n",
    "ds3_train = ds3_train.rename_column('label', 'labels')\n",
    "ds3_test = load_dataset(\"coastalcph/lex_glue\", \"case_hold\", split='test')\n",
    "ds3_test = ds3_test.rename_column('label', 'labels')\n",
    "print(ds3_train)\n",
    "\n",
    "ds4_train = load_dataset(\"coastalcph/lex_glue\", \"ecthr_a\", split='train')\n",
    "ds4_test = load_dataset(\"coastalcph/lex_glue\", \"ecthr_a\", split='test')\n",
    "print(ds4_train)\n",
    "\n",
    "ds5_train = load_dataset(\"coastalcph/lex_glue\", \"ecthr_b\", split='train')\n",
    "ds5_test = load_dataset(\"coastalcph/lex_glue\", \"ecthr_b\", split='test')\n",
    "print(ds5_train)\n",
    "\n",
    "ds6_train = load_dataset(\"coastalcph/lex_glue\", \"eurlex\", split='train')\n",
    "ds6_test = load_dataset(\"coastalcph/lex_glue\", \"eurlex\", split='test')\n",
    "print(ds6_train)\n",
    "\n",
    "ds7_train = load_dataset(\"coastalcph/lex_glue\", \"ledgar\", split='train')\n",
    "ds7_train = ds7_train.rename_column('label', 'labels')\n",
    "ds7_test = load_dataset(\"coastalcph/lex_glue\", \"ledgar\", split='test')\n",
    "ds7_test = ds7_test.rename_column('label', 'labels')\n",
    "print(ds7_train)\n",
    "\n",
    "ds8_train = load_dataset(\"coastalcph/lex_glue\", \"scotus\", split='train')\n",
    "ds8_train = ds8_train.rename_column('label', 'labels')\n",
    "ds8_test = load_dataset(\"coastalcph/lex_glue\", \"scotus\", split='test')\n",
    "ds8_test = ds8_test.rename_column('label', 'labels')\n",
    "print(ds8_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d1aa5e",
   "metadata": {},
   "source": [
    "### Here I am pre-processing the data for the DistilBART model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33c6b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "687cc31e-c81c-4613-891f-7c61aba144cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14301ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function for text and summaries\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(examples['text'], max_length=512, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Tokenize the output summary labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['labels'], max_length=150, truncation=True, padding='max_length')\n",
    "\n",
    "    # Set the tokenized labels in the input dictionary\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "def tokenize_function_for_ds4(examples):\n",
    "    # Tokenize each item in the list of 'text' entries\n",
    "    inputs = tokenizer(\n",
    "        examples['text'],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        is_split_into_words=True  # Add this if each item is already tokenized/split into words\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "def tokenize_function_for_ds6(examples):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(examples['text'], max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "    # If labels are in batches, process accordingly\n",
    "    if 'label' in examples:\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                [str(label) for label in examples['label']],\n",
    "                max_length=150,\n",
    "                truncation=True,\n",
    "                padding='max_length'\n",
    "            )\n",
    "        inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "def tokenize_function_for_ds7(examples):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(examples['text'], max_length=512, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Convert labels to strings if necessary\n",
    "    labels = [str(label) for label in examples['labels']]\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tokenized_labels = tokenizer(labels, max_length=150, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Set the tokenized labels in the input dictionary\n",
    "    inputs['labels'] = tokenized_labels['input_ids']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812264cd",
   "metadata": {},
   "source": [
    "### Here I am just Tokenizing 'ds1' and 'ds2' for DistilBART (ds1_train and ds2_actual)\n",
    "\n",
    "### TODO: Tokenize the training set data later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dfffdba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35ab9154e3e4d74b35dd33c6f972134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e563c235e7a4b9db692508a4fe76a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d7c9fea2374afa960323c2ec9db738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9576bd5b7d974f9380827137a045d851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e4565b096e4cea9f47c80c842c1822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42d52b6de0145288d95d41b3dc7e6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the datasets for DistilBART\n",
    "# Training Data\n",
    "ds1_train_tokenized = ds1_train.map(tokenize_function, batched=True)\n",
    "\n",
    "ds2_tokenized = ds2.map(tokenize_function, batched=True)\n",
    "ds2_tokenized = ds2_tokenized.train_test_split(test_size=0.2)\n",
    "ds2_train_tokenized = ds2_tokenized['train'] \n",
    "\n",
    "# ds3_train_tokenized = ds3_train.map(tokenize_function, batched=True) <-- multiple choice data\n",
    "ds4_train_tokenized = ds4_train.map(tokenize_function_for_ds4, batched=True)\n",
    "ds5_train_tokenized = ds5_train.map(tokenize_function_for_ds4, batched=True)\n",
    "ds6_train_tokenized = ds6_train.map(tokenize_function_for_ds6, batched=True)\n",
    "ds7_train_tokenized = ds7_train.map(tokenize_function_for_ds7, batched=True)\n",
    "ds8_train_tokenized = ds8_train.map(tokenize_function_for_ds7, batched=True)\n",
    "\n",
    "# Testing Data\n",
    "ds1_test_tokenized = ds1_test.map(tokenize_function, batched=True)\n",
    "\n",
    "ds2_test_tokenized = ds2_tokenized['test']\n",
    "\n",
    "# ds3_test_tokenized = ds3_test.map(tokenize_function, batched=True) <-- multiple choice data\n",
    "ds4_test_tokenized = ds4_test.map(tokenize_function_for_ds4, batched=True)\n",
    "ds5_test_tokenized = ds5_test.map(tokenize_function_for_ds4, batched=True)\n",
    "ds6_test_tokenized = ds6_test.map(tokenize_function_for_ds6, batched=True)\n",
    "ds7_test_tokenized = ds7_test.map(tokenize_function_for_ds7, batched=True)\n",
    "ds8_test_tokenized = ds8_test.map(tokenize_function_for_ds7, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "793df597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds1_train_tokenized features: {'text': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "ds2_train_tokenized features: {'text': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "ds4_train_tokenized features: {'text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'labels': Sequence(feature=ClassLabel(names=['2', '3', '5', '6', '8', '9', '10', '11', '14', 'P1-1'], id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "ds5_train_tokenized features: {'text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'labels': Sequence(feature=ClassLabel(names=['2', '3', '5', '6', '8', '9', '10', '11', '14', 'P1-1'], id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "ds6_train_tokenized features: {'text': Value(dtype='string', id=None), 'labels': Sequence(feature=ClassLabel(names=['100163', '100168', '100169', '100170', '100171', '100172', '100173', '100174', '100175', '100176', '100177', '100179', '100180', '100183', '100184', '100185', '100186', '100187', '100189', '100190', '100191', '100192', '100193', '100194', '100195', '100196', '100197', '100198', '100199', '100200', '100201', '100202', '100204', '100205', '100206', '100207', '100212', '100214', '100215', '100220', '100221', '100222', '100223', '100224', '100226', '100227', '100229', '100230', '100231', '100232', '100233', '100234', '100235', '100237', '100238', '100239', '100240', '100241', '100242', '100243', '100244', '100245', '100246', '100247', '100248', '100249', '100250', '100252', '100253', '100254', '100255', '100256', '100257', '100258', '100259', '100260', '100261', '100262', '100263', '100264', '100265', '100266', '100268', '100269', '100270', '100271', '100272', '100273', '100274', '100275', '100276', '100277', '100278', '100279', '100280', '100281', '100282', '100283', '100284', '100285'], id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "ds7_train_tokenized features: {'text': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "ds8_train_tokenized features: {'text': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Checking the features of each dataset\n",
    "print(\"ds1_train_tokenized features:\", ds1_train_tokenized.features)\n",
    "print(\"ds2_train_tokenized features:\", ds2_train_tokenized.features)\n",
    "print(\"ds4_train_tokenized features:\", ds4_train_tokenized.features)\n",
    "print(\"ds5_train_tokenized features:\", ds5_train_tokenized.features)\n",
    "print(\"ds6_train_tokenized features:\", ds6_train_tokenized.features)\n",
    "print(\"ds7_train_tokenized features:\", ds7_train_tokenized.features)\n",
    "print(\"ds8_train_tokenized features:\", ds8_train_tokenized.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca250aac",
   "metadata": {},
   "source": [
    "### Concatenating Tokenized Datasets\n",
    "\n",
    "Based on the schemas of the tokenized datasets, there are differences in the structure of the `text` and `labels` fields:\n",
    "\n",
    "- **`text` Field**:  \n",
    "   - In `ds1_train_tokenized`, `ds2_train_tokenized`, `ds6_train_tokenized`, `ds7_train_tokenized`, and `ds8_train_tokenized`, the `text` field is of type `Value(dtype='string')` (a single string).\n",
    "   - In `ds4_train_tokenized` and `ds5_train_tokenized`, the `text` field is of type `Sequence(feature=Value(dtype='string'))` (a sequence of strings).\n",
    "\n",
    "- **`labels` Field**:  \n",
    "   - In `ds1_train_tokenized`, `ds2_train_tokenized`, `ds7_train_tokenized`, and `ds8_train_tokenized`, the `labels` field is a `Sequence` of `int64` values.\n",
    "   - In `ds4_train_tokenized`, `ds5_train_tokenized`, and `ds6_train_tokenized`, the `labels` field is a `Sequence` of `ClassLabel` objects.\n",
    "\n",
    "### Which Datasets Can Be Concatenated?\n",
    "\n",
    "1. **Datasets with Matching `text` and `labels` Fields:**\n",
    "   - The following datasets have the same `text` and `labels` types and can be concatenated directly:\n",
    "     - `ds1_train_tokenized`\n",
    "     - `ds2_train_tokenized`\n",
    "     - `ds7_train_tokenized`\n",
    "     - `ds8_train_tokenized`\n",
    "\n",
    "   These datasets all have `text` as `Value(dtype='string')` and `labels` as `Sequence(feature=Value(dtype='int64'))`.\n",
    "\n",
    "2. **Datasets with `ClassLabel` in `labels`:**\n",
    "   - The following datasets have `ClassLabel` in the `labels` field and can be concatenated after aligning the `text` field:\n",
    "     - `ds4_train_tokenized`\n",
    "     - `ds5_train_tokenized`\n",
    "     - `ds6_train_tokenized`\n",
    "\n",
    "   Note that `ds4_train_tokenized` and `ds5_train_tokenized` have `text` as `Sequence(feature=Value(dtype='string'))`, while `ds6_train_tokenized` has `text` as `Value(dtype='string')`. You will need to cast these to the same type before concatenating.\n",
    "\n",
    "### How to Concatenate?\n",
    "\n",
    "1. **Concatenating Compatible Datasets Directly**:\n",
    "   You can concatenate the following datasets directly:\n",
    "   \n",
    "   `combined_training_tokenized_dataset = concatenate_datasets([\n",
    "       ds1_train_tokenized, \n",
    "       ds2_train_tokenized,\n",
    "       ds7_train_tokenized,\n",
    "       ds8_train_tokenized\n",
    "   ])\n",
    "    \n",
    "2. **Aligning Features for Other Datasets**:\n",
    "   For datasets with differing `text` fields, they can be casted to a consistent type before concatenating:\n",
    "   \n",
    "   `\n",
    "    ds4_train_tokenized = ds4_train_tokenized.cast({'text': Value('string')})\n",
    "    ds5_train_tokenized = ds5_train_tokenized.cast({'text': Value('string')})\n",
    "    combined_classlabel_tokenized_dataset = concatenate_datasets([\n",
    "        ds4_train_tokenized, \n",
    "        ds5_train_tokenized, \n",
    "        ds6_train_tokenized\n",
    "        ])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bfaa4f",
   "metadata": {},
   "source": [
    "## TODO: choose what datasets to concatenate and how to concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bcfc0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6396db52",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_training_tokenized_dataset = concatenate_datasets([\n",
    "ds1_train_tokenized, \n",
    "ds2_train_tokenized,\n",
    "ds7_train_tokenized,\n",
    "ds8_train_tokenized\n",
    "])\n",
    "\n",
    "combined_testing_tokenized_dataset = concatenate_datasets([\n",
    "ds1_test_tokenized, \n",
    "ds2_test_tokenized,\n",
    "ds7_test_tokenized,\n",
    "ds8_test_tokenized\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd345cbd",
   "metadata": {},
   "source": [
    "### TODO: set the other dataset formats later:\n",
    "\n",
    "### Extra Columns (`input_ids`, `attention_mask`, `labels`)\n",
    "- **`input_ids`**: Token IDs representing the input text for the model.\n",
    "- **`attention_mask`**: Identifies which tokens are real and which are padding.\n",
    "- **`labels`**: Token IDs representing the target summary, used for training.\n",
    "These columns are essential for the model to properly process inputs, ignore padding, and learn to generate correct summaries during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba777233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset format to PyTorch tensors\n",
    "# print(ds1_train_tokenized)\n",
    "combined_training_tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "combined_testing_tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd4efb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4d0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56941f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50e92e97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07068ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "633966e9",
   "metadata": {},
   "source": [
    "### Load the DistilBART model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "17d0ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "64c10551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DistilBART model for conditional generation\n",
    "model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20545c",
   "metadata": {},
   "source": [
    "### Setting up training arguments for the model here\n",
    "\n",
    "### TODO: These can be modified later to improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7344f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6e709e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',            # output directory\n",
    "    eval_strategy=\"epoch\",       # evaluate at each epoch\n",
    "    learning_rate=5e-5,                # learning rate\n",
    "    per_device_train_batch_size=4,     # batch size for training\n",
    "    per_device_eval_batch_size=4,      # batch size for evaluation\n",
    "    num_train_epochs=3,                # number of training epochs\n",
    "    weight_decay=0.01,                 # strength of weight decay\n",
    "    save_total_limit=2,                # only keep last 2 checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c29b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_training_tokenized_dataset,\n",
    "    eval_dataset=combined_testing_tokenized_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3025527f",
   "metadata": {},
   "source": [
    "### Training the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164497d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='54612' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    2/54612 : < :, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e43ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb243a0",
   "metadata": {},
   "source": [
    "### Training and Evaluation Results\n",
    "\n",
    "After training the DistilBART model for **3 epochs** on the legal case summarization dataset, we achieved the following results:\n",
    "\n",
    "#### Training Metrics:\n",
    "- **Training Loss**: **1.8569**\n",
    "  - The training loss represents the average difference between the predicted token probabilities and the actual tokens across the entire dataset. For a complex task like summarization, this loss value indicates that the model is learning effectively.\n",
    "  - While ideally a loss closer to zero is better, for sequence generation tasks involving long and complex legal texts, a value around **1.8** is reasonable. The model is capturing the patterns within the legal data without significant overfitting.\n",
    "\n",
    "#### Evaluation Metrics:\n",
    "- **Evaluation Loss**: **1.9931**\n",
    "  - The evaluation loss is slightly higher than the training loss, which suggests that the model generalizes moderately well to unseen data. This is a positive sign as it implies that the model has not overfit significantly to the training dataset.\n",
    "  - Summarization models, particularly with large input/output sequences and complex legal terminology, typically have evaluation loss values greater than **1**. The small difference between the training and evaluation loss indicates good generalization.\n",
    "\n",
    "- **Evaluation Runtime**: **3,726.99 seconds** (~62 minutes)\n",
    "  - This is the time taken to evaluate the model over the validation set. The runtime is reasonable considering the complexity of the task and the length of the input sequences.\n",
    "\n",
    "- **Samples per Second**:\n",
    "  - **Training**: **0.407** samples per second\n",
    "  - **Evaluation**: **0.417** samples per second\n",
    "  - These rates are consistent across training and evaluation, indicating that the model was trained and evaluated with stable performance given the computational resources. The relatively low samples per second can be attributed to the complexity of processing long legal documents and generating summaries.\n",
    "\n",
    "#### Interpretation of Loss Values:\n",
    "- **Training Loss and Evaluation Loss**:\n",
    "  - The **training loss of 1.8569** compared to the **evaluation loss of 1.9931** indicates that the model is not significantly overfitting to the training set, which is a good outcome. The slight increase in evaluation loss shows that the model is encountering some additional complexity when dealing with unseen data, which is expected.\n",
    "  - In general, for summarization tasks involving complex data, a loss in the range of **1.5 - 3.0** is typical. This is due to the nature of cross-entropy loss accumulating over long sequences of tokens. Thus, the current loss values are quite reasonable.\n",
    "\n",
    "#### Next Steps for Improvement:\n",
    "1. **Hyperparameter Tuning**:\n",
    "   - Consider adjusting the learning rate or using **scheduled learning rate decay** to help further reduce the training and evaluation loss.\n",
    "2. **Additional Training Epochs**:\n",
    "   - Training for an additional **1-2 epochs** could further reduce the loss, provided that overfitting is controlled.\n",
    "3. **Regularization Techniques**:\n",
    "   - **Weight Decay** or **Dropout** could be introduced to help improve generalization.\n",
    "4. **Evaluate with ROUGE Metric**:\n",
    "   - In addition to using loss as a performance measure, evaluating the model with **ROUGE** scores can give a more targeted assessment of how well the summaries capture the important content from the legal texts.\n",
    "\n",
    "#### Summary:\n",
    "- The **training and evaluation losses** are reasonable for a text generation task involving legal documents. The model seems to be learning effectively without significant overfitting.\n",
    "- Further improvement can be achieved through hyperparameter tuning, training for additional epochs, and using metrics such as **ROUGE** to better evaluate the quality of the generated summaries.\n",
    "\n",
    "The next logical step is to test the quality of the generated summaries by comparing them with the reference summaries and calculating relevant metrics to better understand the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cf96ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
