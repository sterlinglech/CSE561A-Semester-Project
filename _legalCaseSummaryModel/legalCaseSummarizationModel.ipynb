{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92190fe1",
   "metadata": {},
   "source": [
    "# Possible models to use\n",
    "\n",
    "## DistilBART - distilled version of BART, which is much smaller than the full BART model but retains much of its performance. Since it is distilled, it's faster and more efficient while still being well-suited for summarization tasks. DistilBART is designed for text summarization, and the cnn-12-6 variant is trained on news articles, making it a viable medium sized model for summarizing legal documents.\n",
    "\n",
    "## T5 (Text-to-Text Transfer Transformer) - Small or Base - T5 treats every task as a text-to-text problem, making it very flexible for summarization. The small and base variants offer a middle ground between performance and model size, making them suitable for use cases where computational resources are limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312bc7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df84a5ae",
   "metadata": {},
   "source": [
    "### Here I load the datasets and edit some of the columns prior to tokenizing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ecc784f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 7773\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n",
      "Dataset({\n",
      "    features: ['context', 'endings', 'labels'],\n",
      "    num_rows: 45000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 9000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 9000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 55000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 60000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "ds1_train = load_dataset(\"joelniklaus/legal_case_document_summarization\", split='train')\n",
    "ds1_train = ds1_train.remove_columns(['dataset_name'])\n",
    "ds1_train = ds1_train.rename_column('judgement', 'text')\n",
    "ds1_train = ds1_train.rename_column('summary', 'labels')\n",
    "print(ds1_train)\n",
    "\n",
    "ds1_test = load_dataset(\"joelniklaus/legal_case_document_summarization\", split='test')\n",
    "ds1_test = ds1_test.remove_columns(['dataset_name'])\n",
    "ds1_test = ds1_test.rename_column('judgement', 'text')\n",
    "ds1_test = ds1_test.rename_column('summary', 'labels')\n",
    "\n",
    "# NOTE: This dataset only has 50 rows. It may not be a dataset we want to use.\n",
    "# NOTE: THIS DATA IS NOT PLAYING NICELY WITH CONCATENATION\n",
    "# Although the summaries appear to be good\n",
    "ds2 = load_dataset(\"manasvikalyan/legal-documents-summary\")\n",
    "ds2 = ds2['data']\n",
    "ds2 = ds2.remove_columns(['summary_a2'])\n",
    "ds2 = ds2.rename_column('summary_a1', 'labels')\n",
    "ds2 = ds2.rename_column('judgement', 'text')\n",
    "print(ds2)\n",
    "\n",
    "# TODO: need to split this dataset manually later\n",
    "\n",
    "# NOTE: This dataset may not be useful the Task: Text Summarization. But moreso, option selection.\n",
    "# Context: is a given legal scenario or fact pattern\n",
    "# Options (Holdings): Multiple candidate holdings, one of which is correct.\n",
    "# Labels: The correct holding is labeled to allow supervised learning and evaluation\n",
    "ds3_train = load_dataset(\"coastalcph/lex_glue\", \"case_hold\", split='train')\n",
    "ds3_train = ds3_train.rename_column('label', 'labels')\n",
    "ds3_test = load_dataset(\"coastalcph/lex_glue\", \"case_hold\", split='test')\n",
    "ds3_test = ds3_test.rename_column('label', 'labels')\n",
    "print(ds3_train)\n",
    "\n",
    "ds4_train = load_dataset(\"coastalcph/lex_glue\", \"ecthr_a\", split='train')\n",
    "ds4_test = load_dataset(\"coastalcph/lex_glue\", \"ecthr_a\", split='test')\n",
    "print(ds4_train)\n",
    "\n",
    "ds5_train = load_dataset(\"coastalcph/lex_glue\", \"ecthr_b\", split='train')\n",
    "ds5_test = load_dataset(\"coastalcph/lex_glue\", \"ecthr_b\", split='test')\n",
    "print(ds5_train)\n",
    "\n",
    "ds6_train = load_dataset(\"coastalcph/lex_glue\", \"eurlex\", split='train')\n",
    "ds6_test = load_dataset(\"coastalcph/lex_glue\", \"eurlex\", split='test')\n",
    "print(ds6_train)\n",
    "\n",
    "ds7_train = load_dataset(\"coastalcph/lex_glue\", \"ledgar\", split='train')\n",
    "ds7_train = ds7_train.rename_column('label', 'labels')\n",
    "ds7_test = load_dataset(\"coastalcph/lex_glue\", \"ledgar\", split='test')\n",
    "ds7_test = ds7_test.rename_column('label', 'labels')\n",
    "print(ds7_train)\n",
    "\n",
    "ds8_train = load_dataset(\"coastalcph/lex_glue\", \"scotus\", split='train')\n",
    "ds8_train = ds8_train.rename_column('label', 'labels')\n",
    "ds8_test = load_dataset(\"coastalcph/lex_glue\", \"scotus\", split='test')\n",
    "ds8_test = ds8_test.rename_column('label', 'labels')\n",
    "print(ds8_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d1aa5e",
   "metadata": {},
   "source": [
    "### Here I am pre-processing the data for the DistilBART model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33c6b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "687cc31e-c81c-4613-891f-7c61aba144cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14301ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function for text and summaries\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(examples['text'], max_length=512, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Tokenize the output summary labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['labels'], max_length=150, truncation=True, padding='max_length')\n",
    "\n",
    "    # Set the tokenized labels in the input dictionary\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "def tokenize_function_for_ds4(examples):\n",
    "    # Tokenize each item in the list of 'text' entries\n",
    "    inputs = tokenizer(\n",
    "        examples['text'],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        is_split_into_words=True  # Add this if each item is already tokenized/split into words\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "def tokenize_function_for_ds6(examples):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(examples['text'], max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "    # If labels are in batches, process accordingly\n",
    "    if 'label' in examples:\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                [str(label) for label in examples['label']],\n",
    "                max_length=150,\n",
    "                truncation=True,\n",
    "                padding='max_length'\n",
    "            )\n",
    "        inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "def tokenize_function_for_ds7(examples):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(examples['text'], max_length=512, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Convert labels to strings if necessary\n",
    "    labels = [str(label) for label in examples['labels']]\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tokenized_labels = tokenizer(labels, max_length=150, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Set the tokenized labels in the input dictionary\n",
    "    inputs['labels'] = tokenized_labels['input_ids']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812264cd",
   "metadata": {},
   "source": [
    "### Here I am just Tokenizing 'ds1' and 'ds2' for DistilBART (ds1_train and ds2_actual)\n",
    "\n",
    "### TODO: Tokenize the training set data later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dfffdba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35ab9154e3e4d74b35dd33c6f972134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e563c235e7a4b9db692508a4fe76a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d7c9fea2374afa960323c2ec9db738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9576bd5b7d974f9380827137a045d851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e4565b096e4cea9f47c80c842c1822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42d52b6de0145288d95d41b3dc7e6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the datasets for DistilBART\n",
    "# Training Data\n",
    "ds1_train_tokenized = ds1_train.map(tokenize_function, batched=True)\n",
    "\n",
    "ds2_tokenized = ds2.map(tokenize_function, batched=True)\n",
    "ds2_tokenized = ds2_tokenized.train_test_split(test_size=0.2)\n",
    "ds2_train_tokenized = ds2_tokenized['train'] \n",
    "\n",
    "# ds3_train_tokenized = ds3_train.map(tokenize_function, batched=True) <-- multiple choice data\n",
    "ds4_train_tokenized = ds4_train.map(tokenize_function_for_ds4, batched=True)\n",
    "ds5_train_tokenized = ds5_train.map(tokenize_function_for_ds4, batched=True)\n",
    "ds6_train_tokenized = ds6_train.map(tokenize_function_for_ds6, batched=True)\n",
    "ds7_train_tokenized = ds7_train.map(tokenize_function_for_ds7, batched=True)\n",
    "ds8_train_tokenized = ds8_train.map(tokenize_function_for_ds7, batched=True)\n",
    "\n",
    "# Testing Data\n",
    "ds1_test_tokenized = ds1_test.map(tokenize_function, batched=True)\n",
    "\n",
    "ds2_test_tokenized = ds2_tokenized['test']\n",
    "\n",
    "# ds3_test_tokenized = ds3_test.map(tokenize_function, batched=True) <-- multiple choice data\n",
    "ds4_test_tokenized = ds4_test.map(tokenize_function_for_ds4, batched=True)\n",
    "ds5_test_tokenized = ds5_test.map(tokenize_function_for_ds4, batched=True)\n",
    "ds6_test_tokenized = ds6_test.map(tokenize_function_for_ds6, batched=True)\n",
    "ds7_test_tokenized = ds7_test.map(tokenize_function_for_ds7, batched=True)\n",
    "ds8_test_tokenized = ds8_test.map(tokenize_function_for_ds7, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d8a17c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds1_train_tokenized features: {'text': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "ds2_train_tokenized features: {'text': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "ds4_train_tokenized features: {'text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'labels': Sequence(feature=ClassLabel(names=['2', '3', '5', '6', '8', '9', '10', '11', '14', 'P1-1'], id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "ds5_train_tokenized features: {'text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'labels': Sequence(feature=ClassLabel(names=['2', '3', '5', '6', '8', '9', '10', '11', '14', 'P1-1'], id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "ds6_train_tokenized features: {'text': Value(dtype='string', id=None), 'labels': Sequence(feature=ClassLabel(names=['100163', '100168', '100169', '100170', '100171', '100172', '100173', '100174', '100175', '100176', '100177', '100179', '100180', '100183', '100184', '100185', '100186', '100187', '100189', '100190', '100191', '100192', '100193', '100194', '100195', '100196', '100197', '100198', '100199', '100200', '100201', '100202', '100204', '100205', '100206', '100207', '100212', '100214', '100215', '100220', '100221', '100222', '100223', '100224', '100226', '100227', '100229', '100230', '100231', '100232', '100233', '100234', '100235', '100237', '100238', '100239', '100240', '100241', '100242', '100243', '100244', '100245', '100246', '100247', '100248', '100249', '100250', '100252', '100253', '100254', '100255', '100256', '100257', '100258', '100259', '100260', '100261', '100262', '100263', '100264', '100265', '100266', '100268', '100269', '100270', '100271', '100272', '100273', '100274', '100275', '100276', '100277', '100278', '100279', '100280', '100281', '100282', '100283', '100284', '100285'], id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "ds7_train_tokenized features: {'text': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "ds8_train_tokenized features: {'text': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Checking the features of each dataset\n",
    "print(\"ds1_train_tokenized features:\", ds1_train_tokenized.features)\n",
    "print(\"ds2_train_tokenized features:\", ds2_train_tokenized.features)\n",
    "print(\"ds4_train_tokenized features:\", ds4_train_tokenized.features)\n",
    "print(\"ds5_train_tokenized features:\", ds5_train_tokenized.features)\n",
    "print(\"ds6_train_tokenized features:\", ds6_train_tokenized.features)\n",
    "print(\"ds7_train_tokenized features:\", ds7_train_tokenized.features)\n",
    "print(\"ds8_train_tokenized features:\", ds8_train_tokenized.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f68b5",
   "metadata": {},
   "source": [
    "### Concatenating Tokenized Datasets\n",
    "\n",
    "Based on the schemas of the tokenized datasets, there are differences in the structure of the `text` and `labels` fields:\n",
    "\n",
    "- **`text` Field**:  \n",
    "   - In `ds1_train_tokenized`, `ds2_train_tokenized`, `ds6_train_tokenized`, `ds7_train_tokenized`, and `ds8_train_tokenized`, the `text` field is of type `Value(dtype='string')` (a single string).\n",
    "   - In `ds4_train_tokenized` and `ds5_train_tokenized`, the `text` field is of type `Sequence(feature=Value(dtype='string'))` (a sequence of strings).\n",
    "\n",
    "- **`labels` Field**:  \n",
    "   - In `ds1_train_tokenized`, `ds2_train_tokenized`, `ds7_train_tokenized`, and `ds8_train_tokenized`, the `labels` field is a `Sequence` of `int64` values.\n",
    "   - In `ds4_train_tokenized`, `ds5_train_tokenized`, and `ds6_train_tokenized`, the `labels` field is a `Sequence` of `ClassLabel` objects.\n",
    "\n",
    "### Which Datasets Can Be Concatenated?\n",
    "\n",
    "1. **Datasets with Matching `text` and `labels` Fields:**\n",
    "   - The following datasets have the same `text` and `labels` types and can be concatenated directly:\n",
    "     - `ds1_train_tokenized`\n",
    "     - `ds2_train_tokenized`\n",
    "     - `ds7_train_tokenized`\n",
    "     - `ds8_train_tokenized`\n",
    "\n",
    "   These datasets all have `text` as `Value(dtype='string')` and `labels` as `Sequence(feature=Value(dtype='int64'))`.\n",
    "\n",
    "2. **Datasets with `ClassLabel` in `labels`:**\n",
    "   - The following datasets have `ClassLabel` in the `labels` field and can be concatenated after aligning the `text` field:\n",
    "     - `ds4_train_tokenized`\n",
    "     - `ds5_train_tokenized`\n",
    "     - `ds6_train_tokenized`\n",
    "\n",
    "   Note that `ds4_train_tokenized` and `ds5_train_tokenized` have `text` as `Sequence(feature=Value(dtype='string'))`, while `ds6_train_tokenized` has `text` as `Value(dtype='string')`. You will need to cast these to the same type before concatenating.\n",
    "\n",
    "### How to Concatenate?\n",
    "\n",
    "1. **Concatenating Compatible Datasets Directly**:\n",
    "   You can concatenate the following datasets directly:\n",
    "   \n",
    "   `combined_training_tokenized_dataset = concatenate_datasets([\n",
    "       ds1_train_tokenized, \n",
    "       ds2_train_tokenized,\n",
    "       ds7_train_tokenized,\n",
    "       ds8_train_tokenized\n",
    "   ])\n",
    "    \n",
    "2. **Aligning Features for Other Datasets**:\n",
    "   For datasets with differing `text` fields, they can be casted to a consistent type before concatenating:\n",
    "   \n",
    "   `\n",
    "    ds4_train_tokenized = ds4_train_tokenized.cast({'text': Value('string')})\n",
    "    ds5_train_tokenized = ds5_train_tokenized.cast({'text': Value('string')})\n",
    "    combined_classlabel_tokenized_dataset = concatenate_datasets([\n",
    "        ds4_train_tokenized, \n",
    "        ds5_train_tokenized, \n",
    "        ds6_train_tokenized\n",
    "        ])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4619f727",
   "metadata": {},
   "source": [
    "## TODO: choose what datasets to concatenate and how to concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae62dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f36cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_training_tokenized_dataset = concatenate_datasets([\n",
    "ds1_train_tokenized, \n",
    "ds2_train_tokenized,\n",
    "ds7_train_tokenized,\n",
    "ds8_train_tokenized\n",
    "])\n",
    "\n",
    "combined_testing_tokenized_dataset = concatenate_datasets([\n",
    "ds1_test_tokenized, \n",
    "ds2_test_tokenized,\n",
    "ds7_test_tokenized,\n",
    "ds8_test_tokenized\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd345cbd",
   "metadata": {},
   "source": [
    "### TODO: set the other dataset formats later:\n",
    "\n",
    "### Extra Columns (`input_ids`, `attention_mask`, `labels`)\n",
    "- **`input_ids`**: Token IDs representing the input text for the model.\n",
    "- **`attention_mask`**: Identifies which tokens are real and which are padding.\n",
    "- **`labels`**: Token IDs representing the target summary, used for training.\n",
    "These columns are essential for the model to properly process inputs, ignore padding, and learn to generate correct summaries during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba777233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset format to PyTorch tensors\n",
    "# print(ds1_train_tokenized)\n",
    "combined_training_tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "combined_testing_tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633966e9",
   "metadata": {},
   "source": [
    "### Load the DistilBART model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "17d0ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "64c10551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DistilBART model for conditional generation\n",
    "model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20545c",
   "metadata": {},
   "source": [
    "### Setting up training arguments for the model here\n",
    "\n",
    "### TODO: These can be modified later to improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7344f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6e709e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',            # output directory\n",
    "    eval_strategy=\"epoch\",       # evaluate at each epoch\n",
    "    learning_rate=5e-5,                # learning rate\n",
    "    per_device_train_batch_size=4,     # batch size for training\n",
    "    per_device_eval_batch_size=4,      # batch size for evaluation\n",
    "    num_train_epochs=3,                # number of training epochs\n",
    "    weight_decay=0.01,                 # strength of weight decay\n",
    "    save_total_limit=2,                # only keep last 2 checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c29b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_training_tokenized_dataset,\n",
    "    eval_dataset=combined_testing_tokenized_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3025527f",
   "metadata": {},
   "source": [
    "### Training the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c164497d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2569' max='54612' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2569/54612 2:39:04 < 53:44:58, 0.27 it/s, Epoch 0.14/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [83], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e43ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe51e3",
   "metadata": {},
   "source": [
    "### Training and Evaluation Results\n",
    "\n",
    "After training the DistilBART model for **3 epochs** on the legal case summarization dataset, we achieved the following results:\n",
    "\n",
    "#### Training Metrics:\n",
    "- **Training Loss**: **1.8569**\n",
    "  - The training loss represents the average difference between the predicted token probabilities and the actual tokens across the entire dataset. For a complex task like summarization, this loss value indicates that the model is learning effectively.\n",
    "  - While ideally a loss closer to zero is better, for sequence generation tasks involving long and complex legal texts, a value around **1.8** is reasonable. The model is capturing the patterns within the legal data without significant overfitting.\n",
    "\n",
    "#### Evaluation Metrics:\n",
    "- **Evaluation Loss**: **1.9931**\n",
    "  - The evaluation loss is slightly higher than the training loss, which suggests that the model generalizes moderately well to unseen data. This is a positive sign as it implies that the model has not overfit significantly to the training dataset.\n",
    "  - Summarization models, particularly with large input/output sequences and complex legal terminology, typically have evaluation loss values greater than **1**. The small difference between the training and evaluation loss indicates good generalization.\n",
    "\n",
    "- **Evaluation Runtime**: **3,726.99 seconds** (~62 minutes)\n",
    "  - This is the time taken to evaluate the model over the validation set. The runtime is reasonable considering the complexity of the task and the length of the input sequences.\n",
    "\n",
    "- **Samples per Second**:\n",
    "  - **Training**: **0.407** samples per second\n",
    "  - **Evaluation**: **0.417** samples per second\n",
    "  - These rates are consistent across training and evaluation, indicating that the model was trained and evaluated with stable performance given the computational resources. The relatively low samples per second can be attributed to the complexity of processing long legal documents and generating summaries.\n",
    "\n",
    "#### Interpretation of Loss Values:\n",
    "- **Training Loss and Evaluation Loss**:\n",
    "  - The **training loss of 1.8569** compared to the **evaluation loss of 1.9931** indicates that the model is not significantly overfitting to the training set, which is a good outcome. The slight increase in evaluation loss shows that the model is encountering some additional complexity when dealing with unseen data, which is expected.\n",
    "  - In general, for summarization tasks involving complex data, a loss in the range of **1.5 - 3.0** is typical. This is due to the nature of cross-entropy loss accumulating over long sequences of tokens. Thus, the current loss values are quite reasonable.\n",
    "\n",
    "#### Next Steps for Improvement:\n",
    "1. **Hyperparameter Tuning**:\n",
    "   - Consider adjusting the learning rate or using **scheduled learning rate decay** to help further reduce the training and evaluation loss.\n",
    "2. **Additional Training Epochs**:\n",
    "   - Training for an additional **1-2 epochs** could further reduce the loss, provided that overfitting is controlled.\n",
    "3. **Regularization Techniques**:\n",
    "   - **Weight Decay** or **Dropout** could be introduced to help improve generalization.\n",
    "4. **Evaluate with ROUGE Metric**:\n",
    "   - In addition to using loss as a performance measure, evaluating the model with **ROUGE** scores can give a more targeted assessment of how well the summaries capture the important content from the legal texts.\n",
    "\n",
    "#### Summary:\n",
    "- The **training and evaluation losses** are reasonable for a text generation task involving legal documents. The model seems to be learning effectively without significant overfitting.\n",
    "- Further improvement can be achieved through hyperparameter tuning, training for additional epochs, and using metrics such as **ROUGE** to better evaluate the quality of the generated summaries.\n",
    "\n",
    "The next logical step is to test the quality of the generated summaries by comparing them with the reference summaries and calculating relevant metrics to better understand the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c8b96",
   "metadata": {},
   "source": [
    "# Training with an NVIDIA H100 SXM \n",
    "\n",
    "### Model Training and Evaluation Summary\n",
    "\n",
    "After training the model for **3 epochs** on the legal case summarization dataset, we achieved the following results:\n",
    "\n",
    "#### Training Metrics:\n",
    "- **Training Loss**:\n",
    "  - **Epoch 1**: 0.278700\n",
    "  - **Epoch 2**: 0.246900\n",
    "  - **Epoch 3**: 0.125400\n",
    "  - The training loss consistently decreased across epochs, which indicates that the model is effectively learning from the dataset. This is a positive trend, especially for a sequence generation task, as the model is gradually fitting the data with better accuracy over time.\n",
    "\n",
    "#### Evaluation Metrics:\n",
    "- **Validation Loss**:\n",
    "  - **Epoch 1**: 0.088182\n",
    "  - **Epoch 2**: 0.141818\n",
    "  - **Epoch 3**: 0.117075\n",
    "  - The validation loss shows a slight fluctuation between epochs, with the lowest loss occurring in Epoch 1. The higher values in Epoch 2 and 3 indicate that the model may be encountering some complexity in generalizing, though it still performs well overall.\n",
    "\n",
    "- **Evaluation Loss**: **0.1170758718937302**\n",
    "  - The evaluation loss suggests that the model generalizes well to unseen data. The small difference between training and evaluation losses demonstrates good generalization, which is a positive sign, as it implies the model is not overfitting.\n",
    "\n",
    "- **Evaluation Runtime**: **90.6238 seconds** (~1.5 minutes)\n",
    "  - The evaluation process took about 90 seconds, which is quite efficient given the complexity of the model and dataset.\n",
    "\n",
    "- **Samples per Second**:\n",
    "  - **Training**: **36.789** samples per second\n",
    "  - **Evaluation**: **128.112** samples per second\n",
    "  - The evaluation speed is significantly higher than the training speed, likely due to the larger computational requirements for backpropagation during training. Both rates suggest the model was trained and evaluated efficiently.\n",
    "\n",
    "#### Interpretation of Loss Values:\n",
    "- **Training Loss and Validation Loss**:\n",
    "  - The training loss consistently decreases, while the validation loss stabilizes. This is indicative of a well-performing model with minimal overfitting. Given that the validation loss is close to the training loss, the model seems to generalize well to unseen data, which is crucial for real-world applications.\n",
    "\n",
    "#### Next Steps for Improvement:\n",
    "1. **Hyperparameter Tuning**:\n",
    "   - Adjust the learning rate or batch size to see if further reductions in loss are possible.\n",
    "2. **Additional Training Epochs**:\n",
    "   - Training for an additional **1-2 epochs** could help reduce the loss further, though care should be taken to avoid overfitting.\n",
    "3. **Regularization Techniques**:\n",
    "   - Techniques such as **Dropout** or **Weight Decay** could be introduced to further improve generalization and reduce validation loss.\n",
    "4. **Advanced Evaluation Metrics**:\n",
    "   - Consider using additional metrics like **ROUGE** or **BLEU** to evaluate the model's performance in terms of summarization quality, not just loss.\n",
    "\n",
    "#### Summary:\n",
    "- The **training and validation losses** are promising for a complex text generation task. The model is learning effectively and generalizing well to the validation set without signs of significant overfitting.\n",
    "- Further improvements could be made by fine-tuning hyperparameters, increasing the number of training epochs, and using advanced evaluation metrics such as ROUGE or BLEU.\n",
    "\n",
    "Next, we can analyze the quality of the generated summaries against reference summaries to further assess the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b7e3e9",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------\n",
    "# Example 107:\n",
    "## Original Summary: \n",
    "On 9 September 2004 the appellant, Steven Allison, was convicted after trial in the High Court at Glasgow of four contraventions of section 4(3)(b) of the Misuse of Drugs Act 1971.\n",
    "In effect, he was found guilty of being concerned in the supplying of cocaine and three other controlled drugs at his home in Cumbernauld, at an address in Falkirk and elsewhere in the United Kingdom, between 12 November and 3 December 2003.\n",
    "The trial judge, Lord Bracadale, sentenced him to 8 years imprisonment.\n",
    "The appellant appealed against both his conviction and sentence.\n",
    "On 7 November 2008 the appeal court (Lord Osborne, Lady Paton and Lord Philip) refused his appeal against conviction, leaving his appeal against sentence to be heard on a date to be fixed.\n",
    "Among his grounds of appeal against conviction was one which was first advanced in an additional Note of Appeal.\n",
    "It relates to the record of a police interview of a John Stronach.\n",
    "Mr Stronach had died before the trial and the Crown introduced the interview into evidence in accordance with the procedure in section 259(5) of the Criminal Procedure (Scotland) Act 1995.\n",
    "Neither before nor during the trial did the Crown disclose to the defence that Mr Stronach had a number of previous convictions and outstanding charges.\n",
    "In particular, he had convictions for reset, theft by opening lockfast places, assault and robbery and assault and breach of the peace.\n",
    "He also had a number of outstanding charges, including two alleged contraventions of the Misuse of Drugs Act 1971, an alleged theft by housebreaking and several alleged contraventions of the Road Traffic Act 1988.\n",
    "One of the outstanding cases under the Misuse of Drugs Act related to events covered by the trial and was known to the appellants legal advisers.\n",
    "The Crown disclosed the previous convictions and the other outstanding charges only while the appellants appeal was pending before the appeal court.\n",
    "This prompted the appellant to lodge his additional ground of appeal: The failure on the part of the Crown to disclose to the defence the existence of all the previous convictions and outstanding charges resulted in the defence being unable to prepare and properly conduct their defence and the result was that the appellant did not receive a fair trial, as guaranteed by article 6(1) of the European Convention on Human Rights.\n",
    "Following the dismissal of his appeal by the appeal court, the appellant applied for leave to appeal to the Privy Council in relation to the additional ground of appeal.\n",
    "On 6 March 2009 the appeal\n",
    "*****************************************************************************************\n",
    "## Generated Summary: \n",
    "On 9 September 2004 Steven Allison was convicted of four contraventions of section 4(3)(b) of the Misuse of Drugs Act 1971.\n",
    "He was found guilty of being concerned in the supply of cocaine and three other controlled drugs at his home in Cumbernauld, at an address in the United Kingdom, between 12 November and 3 December 2003.\n",
    "The trial judge, Lord Bracadale, sentenced him to 8 years imprisonment.\n",
    "In appeal, the appeal court refused his appeal against conviction, leaving his appeal to be heard on a date to be fixed.\n",
    "One of the outstanding cases under the misuse of drugs Act 1971 related to events covered by the trial and was known to the appellants legal advisers.\n",
    "\n",
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f019a0de",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------\n",
    "# Example 2341:\n",
    "## Original Summary: \n",
    "THIS JOINDER SHALL BE CONSTRUED AND ENFORCED IN ACCORDANCE WITH AND GOVERNED BY THE LAW OF THE STATE OF NEW YORK WITHOUT REFERENCE TO ITS CONFLICT OF LAWS PRINCIPLES TO THE EXTENT THAT THE SAME ARE NOT MANDATORILY APPLICABLE BY STATUTE AND WOULD PERMIT OR REQUIRE THE APPLICATION OF LAWS OF ANOTHER JURISDICTION.\n",
    "*****************************************************************************************\n",
    "## Generated Summary: \n",
    "23 This appeal arises out of proceedings under the United Commonwealth ( Commonwealth of Delaware and Commonwealth of Pennsylvania) and is directed to be governed by the provisions of the, and is governed by that law by the United State of Delaware (the Delaware Act), which is a well\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b559f2",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------\n",
    "# Example 10:\n",
    "## Original Summary: \n",
    "Appeal No. 251 of 1963.\n",
    "Appeal by special leave from the judgment and order dated March 20, 1957, of the Patna High Court in Civil Revision No. 40 of 1956.\n",
    "M. C. Setalvad, and R. C. Prasad, for the appellants.\n",
    "The respondent did not appear.\n",
    "March 24, 1964.\n",
    "The short question which arises in this appeal is whether the term \"wages\" as defined by section 2(vi) of the (No. 4 of 1936) (hereinafter called 'the Act ') includes wages fixed by an award in an industrial dispute between the employer and his employees.\n",
    "This question has to be answered in the light of the definition prescribed by section 2(vi) before it was amended in 1958.\n",
    "The subsequent amendment expressly provides by section 2(vi) (a) that any remuneration payable under any award or settlement between the parties or order of a Court, would be included in the main definition under section 2(vi).\n",
    "The point which we have to decide in the present appeal is whether the remuneration payable under an award was not already included in the definition of wages before the said definition was amended.\n",
    "It is common ground that between the appellant, Sasamusa Sugar Works Ltd., and its workmen, the respondents, an award had been made by an Industrial Tribunal fixing the pay of the employees at Rs. 2/2/ per day, and in pursuance of the said award, the management of the appellant had entered into an agreement with the respondents that effect would be given to the wage structure, prescribed by the said award.\n",
    "This agreement was subsequently published in the Bihar Gazette as a part of the award.\n",
    "In spite of the award and the agreement, the appellant paid its employees only As.\n",
    "/ 10 / per day and that led to the present claim made by the respondents under section 15 of the Act.\n",
    "The respondents contended before the payment of wages authority that the refusal of the appellant to pay to them wages at the rate awarded, in substance, amounted to an illegal deduction from their wages and on that basis, they asked for an order from the authority directing the appellant to pay to the respondents the said prescribed wages.\n",
    "The appellant raised two pleas against the respondents'claim.\n",
    "It urged that section 15 of the Act was inapplicable, because the rates of wages fixed by the award did not fall within the definition of wages prescribed by section 2(\n",
    "*****************************************************************************************\n",
    "## Generated Summary:\n",
    "An award had been made by an Industrial Tribunal fixing the pay of the employees at Rs. 2/2/ per day and in pursuance thereof the management of the appellant had entered into an agreement with the respondents that effect would be given to the wage structure prescribed by the said award.\n",
    "This agreement was subsequently published in the Bihar Gazette as a part of the award and the appellant paid its employees only As.\n",
    "/10/ per Day.\n",
    "The respondents contended before the payment of wages authority that the appellant's refusal to pay to them wages at the rate awarded, in substance, amounted to an illegal deduction from their wages and on that basis, they asked for an order from the authority directing the appellant to pay the said"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e944ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
