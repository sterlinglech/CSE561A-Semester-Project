{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b396a93a-ad3a-4e9a-a494-41e38e4bc171",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d33f29b-4437-4a9e-98e2-4696463c6b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 7773\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "ds1_train = load_dataset(\"joelniklaus/legal_case_document_summarization\", split='train')\n",
    "ds1_train = ds1_train.remove_columns(['dataset_name'])\n",
    "ds1_train = ds1_train.rename_column('judgement', 'text')\n",
    "ds1_train = ds1_train.rename_column('summary', 'labels')\n",
    "print(ds1_train)\n",
    "\n",
    "ds1_test = load_dataset(\"joelniklaus/legal_case_document_summarization\", split='test')\n",
    "ds1_test = ds1_test.remove_columns(['dataset_name'])\n",
    "ds1_test = ds1_test.rename_column('judgement', 'text')\n",
    "ds1_test = ds1_test.rename_column('summary', 'labels')\n",
    "\n",
    "# NOTE: This dataset only has 50 rows. It may not be a dataset we want to use.\n",
    "# NOTE: THIS DATA IS NOT PLAYING NICELY WITH CONCATENATION\n",
    "# Although the summaries appear to be good\n",
    "ds2 = load_dataset(\"manasvikalyan/legal-documents-summary\")\n",
    "ds2 = ds2['data']\n",
    "ds2 = ds2.remove_columns(['summary_a2'])\n",
    "ds2 = ds2.rename_column('summary_a1', 'labels')\n",
    "ds2 = ds2.rename_column('judgement', 'text')\n",
    "print(ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f987de1-e338-487e-bb43-e73329fb1b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['judgement', 'dataset_name', 'summary', 'text'],\n",
      "    num_rows: 7773\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ds9: AjayMukundS/Legal_Text_Summarization-llama2\n",
    "ds9_train = load_dataset(\"AjayMukundS/Legal_Text_Summarization-llama2\", split='train')\n",
    "ds9_test = load_dataset(\"AjayMukundS/Legal_Text_Summarization-llama2\", split='test')\n",
    "print(ds9_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0646ec4b-3108-4b55-9101-034b4bafc787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e2be482-9f03-4826-a46a-8fe9f16e65ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6108a129-9e5e-420f-9183-e8701549ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function for text and summaries\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(examples['text'], max_length=512, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Tokenize the output summary labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['labels'], max_length=150, truncation=True, padding='max_length')\n",
    "\n",
    "    # Set the tokenized labels in the input dictionary\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d1d1246-ace2-4369-8dbe-d71ea4ead684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example tokenizer for ds9\n",
    "def tokenize_batch(batch):\n",
    "    # Tokenizing input and target sequences\n",
    "    inputs = tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=1024)\n",
    "    targets = tokenizer(batch['summary'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    \n",
    "    # Assign labels as the target input_ids\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08cf9f69-0a30-4bae-9a2f-3ab3361396d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the datasets for DistilBART\n",
    "# Training Data\n",
    "ds1_train_tokenized = ds1_train.map(tokenize_function, batched=True)\n",
    "\n",
    "ds2_tokenized = ds2.map(tokenize_function, batched=True)\n",
    "ds2_tokenized = ds2_tokenized.train_test_split(test_size=0.2)\n",
    "ds2_train_tokenized = ds2_tokenized['train'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae33d4f-d8ec-4d20-aa2d-49d5bad72da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds9_train_tokenized = ds9_train.map(tokenize_batch, batched=True)\n",
    "ds9_test_tokenized = ds9_test.map(tokenize_batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "354dca5e-5b01-469d-bd65-79fb3a866bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['judgement', 'dataset_name', 'summary', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 30\n",
      "})\n",
      "Dataset({\n",
      "    features: ['judgement', 'dataset_name', 'summary', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Taking 10 examples from each tokenized set to test the training.\n",
    "train_sample = ds9_train_tokenized.select(range(30))\n",
    "test_sample = ds9_test_tokenized.select(range(10))\n",
    "\n",
    "print(train_sample)\n",
    "print(test_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21b2d3db-7cb7-43d8-98e3-af522501e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83cfdcd-afa0-4d35-bc9e-319da31eefa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_training_tokenized_dataset = concatenate_datasets([\n",
    "ds1_train_tokenized, \n",
    "ds2_train_tokenized\n",
    "])\n",
    "\n",
    "combined_testing_tokenized_dataset = concatenate_datasets([\n",
    "ds1_test_tokenized, \n",
    "ds2_test_tokenized\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e85a5-531c-4fd6-ab91-86aa10bc05f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset format to PyTorch tensors\n",
    "# print(ds1_train_tokenized)\n",
    "combined_training_tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "combined_testing_tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e79eabc6-8269-4a5e-8d08-c2a9bebbbe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99917024-5ed1-47c8-a126-ba778b2491fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DistilBART model for conditional generation\n",
    "model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5ade4c9-52ae-427d-9b75-e9971faf95b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To handle padding dynamically (i.e., pad to the longest sequence in a batch rather than a fixed length)\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a28dd2e9-1a21-483f-9b94-416e052f243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5337b244-e29c-4757-a75c-96019d0b5955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerator is using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "print(f\"Accelerator is using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e23318af-3b58-486c-995a-2d34d2eec405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results_testing_with_ds9\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    save_steps=1000,\n",
    "    eval_steps=500,\n",
    "    num_train_epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4fa073f-774f-4ba5-bdab-72d0e305fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample, # change these to real data when done testing with samples\n",
    "    eval_dataset=test_sample,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83593ca1-a99d-4959-8c46-fa0d7aedd83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: dataset_name, text, summary, judgement. If dataset_name, text, summary, judgement are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 30\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 24\n",
      "  Number of trainable parameters = 305,510,400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 06:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_testing_with_ds9/checkpoint-24\n",
      "Configuration saved in ./results_testing_with_ds9/checkpoint-24/config.json\n",
      "Configuration saved in ./results_testing_with_ds9/checkpoint-24/generation_config.json\n",
      "Model weights saved in ./results_testing_with_ds9/checkpoint-24/model.safetensors\n",
      "tokenizer config file saved in ./results_testing_with_ds9/checkpoint-24/tokenizer_config.json\n",
      "Special tokens file saved in ./results_testing_with_ds9/checkpoint-24/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24, training_loss=0.0, metrics={'train_runtime': 412.5234, 'train_samples_per_second': 0.218, 'train_steps_per_second': 0.058, 'total_flos': 139312087695360.0, 'train_loss': 0.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set verbosity to info to see the logs in real time\n",
    "logging.set_verbosity_info()\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3e599d7-8c19-42db-99b2-9c86e332e83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: dataset_name, text, summary, judgement. If dataset_name, text, summary, judgement are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 6.8282, 'eval_samples_per_second': 1.465, 'eval_steps_per_second': 0.439, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f887de8-24f6-44ca-b9dc-79a9533b4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Get a random index\n",
    "random_index = random.randint(0, len(ds9_test_tokenized) - 1)\n",
    "\n",
    "# Access the 'text' property of the randomly selected sample\n",
    "random_sample_text = ds9_test_tokenized[random_index]['text']\n",
    "print(random_sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1f9ae02-7bf0-44b9-a803-aad8da06c86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of generating a summary\n",
    "\n",
    "# Ensure model is on MPS\n",
    "model.to(\"mps\")\n",
    "\n",
    "# Tokenize the text\n",
    "# Currently passing a random_sample_text from ds9_test_tokenized\n",
    "inputs = tokenizer(random_sample_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Move input tensors to MPS\n",
    "inputs = {key: value.to(\"mps\") for key, value in inputs.items()}\n",
    "\n",
    "# Generate the summary with minimum length and check summary_ids\n",
    "summary_ids = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_length=142,           # Based on the config\n",
    "    min_length=56,            # Based on the config\n",
    "    num_beams=4,              # Config default\n",
    "    length_penalty=2.0,        # From the config\n",
    "    no_repeat_ngram_size=3,    # Config default\n",
    "    early_stopping=True        # From config to stop generation early\n",
    ")\n",
    "\n",
    "\n",
    "# Decode the summary and move to CPU before printing\n",
    "summary = tokenizer.decode(summary_ids[0].cpu(), skip_special_tokens=True)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec67ce3-e070-4fe7-96bb-2b5bee1bff8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813cde49-4ef6-4627-81aa-37f9d6d7249e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d529f20e-5abb-4fc5-b6b9-284fd4a6b7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ba3825d-bf96-4b5f-9441-15bb9f37e61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 0, 0, 0, 0, 0, 2]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(summary_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f58d7-c8c0-4115-80ed-8b42db017e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61e26322-a5f8-4e62-a529-276c3a5f0c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     0, 10975,  ..., 10127, 13161,     2]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c040032-2d55-40ee-9e7a-a1b920379db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartConfig {\n",
      "  \"_name_or_path\": \"sshleifer/distilbart-cnn-12-6\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": null,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": null,\n",
      "  \"max_length\": null,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": null,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": null,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": null,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e69bec9-c0b9-40d5-bf89-e9a02f016cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model\n",
    "model.save_pretrained('./trained_HX100_model_1')\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained('./trained_HX100_model_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab2f49-0d1b-42a5-9331-a913a61ee30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('./trained_HX100_model_1')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./trained_HX100_model_1')\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73a2dbe-1358-4dbb-9431-c9e9c495c276",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Select 5 random indices from the combined_testing_tokenized_dataset\n",
    "random_indices = random.sample(range(len(combined_testing_tokenized_dataset)), 5)\n",
    "\n",
    "for idx in random_indices:\n",
    "    # Example: Take the tokenized input from the tokenized testing dataset\n",
    "    example_input_ids = combined_testing_tokenized_dataset['input_ids'][idx]\n",
    "    \n",
    "    # Convert to torch tensor and move to the device (GPU or CPU)\n",
    "    example_input_ids = torch.tensor(example_input_ids).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate the summary using the model\n",
    "    summary_ids = model.generate(example_input_ids, num_beams=4, max_length=200, early_stopping=True)\n",
    "    \n",
    "    # Decode the generated summary\n",
    "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Decode the actual (original) input summary\n",
    "    original_summary = tokenizer.decode(example_input_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Print both summaries for comparison\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(f\"Example {idx + 1}:\")\n",
    "    print(\"Original Summary:\", original_summary)\n",
    "    print(\"*****************************************************************************************\")\n",
    "    print(\"Generated Summary:\", generated_summary)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d84ca-3231-44f4-b840-1d336bbd466d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb33e4dc-3dd9-4e51-a0bb-c8ffecb4863e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a343d3-ac12-44c7-9b68-106120a17fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
